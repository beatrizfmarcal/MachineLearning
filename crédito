python3
# -*- coding: utf-8 -*-

import pandas as pd
'''import matplotlib.pyplot as plt'''
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier

BaseDeDados = pd.read_csv('Credito.csv', sep=';', encoding='latin-1')
Previsores = BaseDeDados.iloc[:,0:19].values
Classe = BaseDeDados.iloc[:,19].values

Classe1 = BaseDeDados.iloc[:,19]
BomPagador = 0
MauPagador = 0
for instancia in Classe1:
    if instancia == 'bom':
        BomPagador += 1
    else:
        MauPagador += 1
print('''A quantidade de bons pagadores é de {}% e a de maus pagadores 
é de {}%.'''.format((BomPagador/1000*100), (MauPagador/1000*100)))

LabelEncoder = LabelEncoder()
Previsores[:,0] = LabelEncoder.fit_transform(Previsores[:,0])
Previsores[:,2] = LabelEncoder.fit_transform(Previsores[:,2])
Previsores[:,3] = LabelEncoder.fit_transform(Previsores[:,3])
Previsores[:,5] = LabelEncoder.fit_transform(Previsores[:,5])
Previsores[:,6] = LabelEncoder.fit_transform(Previsores[:,6])
Previsores[:,8] = LabelEncoder.fit_transform(Previsores[:,8])
Previsores[:,9] = LabelEncoder.fit_transform(Previsores[:,9])
Previsores[:,11] = LabelEncoder.fit_transform(Previsores[:,11])
Previsores[:,13] = LabelEncoder.fit_transform(Previsores[:,13])
Previsores[:,14] = LabelEncoder.fit_transform(Previsores[:,14])
Previsores[:,16] = LabelEncoder.fit_transform(Previsores[:,16])
Previsores[:,18] = LabelEncoder.fit_transform(Previsores[:,18])

# Histogramas. O 3º e o 4º são uma tentativa de melhorar a laber do eixo x
'''plt.figure(1)
plt.subplot(2, 2, 1)
plt.hist(BaseDeDados.CHEQUEESPECIAL)
plt.title('Cheque Especial')
plt.subplot(2, 2, 2)
plt.hist(BaseDeDados.USO_CREDITO)
plt.title('Uso de Crédito')
plt.subplot(2, 2, 3)
plt.hist(Previsores[:,2])
plt.title('Histórico de Crédito')
plt.subplot(2, 2, 4)
plt.hist(Previsores[:,3])
plt.title('Propósito')
plt.tight_layout()'''

X_Treinamento, X_Teste, Y_Treinamento, Y_Teste = train_test_split(Previsores,
                                                                  Classe,
                                                                  test_size=0.3,
                                                                  random_state=0)
NaiveBayes = GaussianNB()
NaiveBayes.fit(X_Treinamento, Y_Treinamento)
Previsoes = NaiveBayes.predict(X_Teste)

Confusao = confusion_matrix(Y_Teste, Previsoes)
TaxaDeAcerto = accuracy_score(Y_Teste, Previsoes)

ETC = ExtraTreesClassifier()
ETC.fit(X_Treinamento, Y_Treinamento)
Importancias = ETC.feature_importances_ 

X_Treinamento2 = X_Treinamento[:, [0, 1, 2, 3, 4, 5, 6, 11, 12]]
X_Teste2 = X_Teste[:, [0, 1, 2, 3, 4, 5, 6, 11, 12]]

NaiveBayes2 = GaussianNB()
NaiveBayes2.fit(X_Treinamento2, Y_Treinamento)
Previsoes2 = NaiveBayes2.predict(X_Teste2)
TaxaDeAcerto2 = accuracy_score(Y_Teste, Previsoes2)

Floresta = RandomForestClassifier()
Floresta.fit(X_Treinamento2, Y_Treinamento)
Previsoes3 = Floresta.predict(X_Teste2)
TaxaDeAcerto3 = accuracy_score(Y_Teste, Previsoes3)
print('''A taxa de erro após a implementação do modelo de Machine Learning
foi de {:.2f}%.'''.format((1 - TaxaDeAcerto3)*100))
